import { systemPrompt } from "../../../lib/systemPrompt";
import { checkAndIncrementLimit } from "../../../lib/chatLimit";
import { getUserId } from "../../../lib/userId";
import { analyzeUserState } from "../../../lib/analyzeUserState";
import { buildSystemPrompt } from "../../../lib/buildSystemPrompt";
import { updatePseudoMemory } from "../../../lib/updatePseudoMemory";
import { getPseudoMemory } from "../../../lib/getPseudoMemory";

import { detectConversationMode } from "../../../lib/conversation/detectConversationMode";
import { modeInstructions } from "../../../lib/conversation/conversationModes";

import OpenAI from "openai";

export const runtime = "nodejs";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

const MAX_HISTORY = 20;

/* =========================
   STYL â€“ TRYB ROZMOWY
   ========================= */
const STYLE_ANCHOR = `
JesteÅ› NaviMind.
JesteÅ› partnerem do rozmowy, nie terapeutÄ….
Pomagasz nazwaÄ‡ sedno sytuacji.
Nie moralizujesz. Nie przesÅ‚uchujesz.
`;

/* =========================
   STYL â€“ TRYB PRAKTYCZNY
   ========================= */
const PRACTICAL_ANCHOR = `
JesteÅ› NaviMind.
UÅ¼ytkownik zadaÅ‚ pytanie techniczne lub praktyczne.
Odpowiadasz konkretnie i rzeczowo.
Nie analizujesz emocji.
Nie zadajesz pytaÅ„ zwrotnych.
`;

/* =========================
   FALLBACK (TYLKO ROZMOWA)
   ========================= */
const FALLBACK_SENTENCE =
  "ChcÄ™ dobrze zrozumieÄ‡ â€” powiedz proszÄ™, co jest teraz dla Ciebie najwaÅ¼niejsze.";

export async function POST(req: Request) {
  try {
    const body = await req.json();
    const { messages, hiddenContext } = body;

    if (!Array.isArray(messages)) {
      throw new Error("messages is not an array");
    }

    /* =========================
       USER + LIMIT
       ========================= */
    const userId = getUserId();
    const limit = await checkAndIncrementLimit(userId);

    if (!limit.allowed) {
      return Response.json(
        {
          error: "LIMIT_REACHED",
          text: "Limit demo zostaÅ‚ osiÄ…gniÄ™ty ðŸ”’",
          limit,
        },
        { status: 429 }
      );
    }

    /* =========================
       HISTORIA (BASIC)
       ========================= */
    const history = messages
      .filter(
        (m: any) =>
          m &&
          (m.role === "user" || m.role === "assistant") &&
          typeof m.content === "string"
      )
      .slice(-MAX_HISTORY);

    const lastUserMessage =
      history.filter((m: any) => m.role === "user").slice(-1)[0]?.content || "";

    /* =========================
       ðŸ”¥ DETEKCJA PYTANIA PRAKTYCZNEGO
       ========================= */
    const isPractical = /^(jak|co|ile|gdzie|kiedy|czy|zrob|sprawdz)\b/i.test(
      lastUserMessage.trim()
    );

    /* =====================================================
       ðŸ”¥ðŸ”¥ TRYB PRAKTYCZNY â€” EARLY RETURN (KONIEC)
       ===================================================== */
    if (isPractical) {
      const completion = await openai.chat.completions.create({
        model: "gpt-4.1-mini",
        temperature: 0.2,
        messages: [
          { role: "system", content: PRACTICAL_ANCHOR },
          { role: "user", content: lastUserMessage },
        ],
      });

      const text =
        completion.choices[0]?.message?.content?.trim() ||
        "SprawdÅº status deploya w panelu Vercel.";

      return Response.json({
        text,
        limit,
        uiHints: {
          isPractical: true,
        },
      });
    }

    /* =========================
       TRYB ROZMOWY (NORMALNY)
       ========================= */
    const analysis = await analyzeUserState(history);
    const conversationMode = detectConversationMode(
      lastUserMessage,
      analysis
    );
    const modePrompt = modeInstructions[conversationMode];

    const rawMemory = await getPseudoMemory(userId);
    const memory = rawMemory ?? { visits: 0 };

    const enrichedSystemPrompt = buildSystemPrompt(
      systemPrompt + "\n\nTRYB ROZMOWY:\n" + modePrompt,
      analysis,
      memory
    );

    await updatePseudoMemory(userId, analysis);

    const documentContext = hiddenContext
      ? {
          role: "system",
          content:
            "UÅ¼ytkownik udostÄ™pniÅ‚ dokument PDF.\n" +
            "Traktuj go jako kontekst.\n" +
            "Odpowiadaj tylko na to, o co pyta.\n\n" +
            hiddenContext.slice(0, 12000),
        }
      : null;

    const completion = await openai.chat.completions.create({
      model: "gpt-4.1-mini",
      temperature: 0.7,
      messages: [
        { role: "system", content: STYLE_ANCHOR },
        { role: "system", content: enrichedSystemPrompt },
        ...(documentContext ? [documentContext] : []),
        ...history,
      ] as any,
    });

    let text =
      completion.choices[0]?.message?.content?.trim() || "";

    if (!text || text.length < 10) {
      text = FALLBACK_SENTENCE;
    }

    return Response.json({
      text,
      limit,
      uiHints: {
        conversationMode,
        isPractical: false,
      },
    });
  } catch (error) {
    console.error("CHAT API ERROR:", error);
    return Response.json(
      { text: "CoÅ› siÄ™ wysypaÅ‚o po stronie serwera." },
      { status: 500 }
    );
  }
}